{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bdceff18",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2023-11-24T21:35:23.397313Z",
     "iopub.status.busy": "2023-11-24T21:35:23.396474Z",
     "iopub.status.idle": "2023-11-24T21:35:36.508827Z",
     "shell.execute_reply": "2023-11-24T21:35:36.508033Z"
    },
    "papermill": {
     "duration": 13.121723,
     "end_time": "2023-11-24T21:35:36.510989",
     "exception": false,
     "start_time": "2023-11-24T21:35:23.389266",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.24.3\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense\n",
    "from tensorflow.keras.preprocessing.sequence import TimeseriesGenerator\n",
    "from tensorflow.keras.metrics import BinaryAccuracy\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.impute import SimpleImputer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bd70886c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-24T21:35:36.523359Z",
     "iopub.status.busy": "2023-11-24T21:35:36.522828Z",
     "iopub.status.idle": "2023-11-24T21:35:37.090535Z",
     "shell.execute_reply": "2023-11-24T21:35:37.089653Z"
    },
    "papermill": {
     "duration": 0.576191,
     "end_time": "2023-11-24T21:35:37.092891",
     "exception": false,
     "start_time": "2023-11-24T21:35:36.516700",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('/kaggle/input/weather-dataset-rattle-package/weatherAUS.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a6789cc4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-24T21:35:37.122521Z",
     "iopub.status.busy": "2023-11-24T21:35:37.121884Z",
     "iopub.status.idle": "2023-11-24T21:35:37.363058Z",
     "shell.execute_reply": "2023-11-24T21:35:37.362075Z"
    },
    "papermill": {
     "duration": 0.250181,
     "end_time": "2023-11-24T21:35:37.365259",
     "exception": false,
     "start_time": "2023-11-24T21:35:37.115078",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df.RainTomorrow.replace({'Yes':1, 'No':0}, inplace=True)\n",
    "df.RainToday.replace({'Yes':1, 'No':0}, inplace=True)\n",
    "df.Date = pd.to_datetime(df.Date)\n",
    "df.dropna(subset='RainTomorrow', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f6ca66b1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-24T21:35:37.377170Z",
     "iopub.status.busy": "2023-11-24T21:35:37.376886Z",
     "iopub.status.idle": "2023-11-24T21:35:37.455136Z",
     "shell.execute_reply": "2023-11-24T21:35:37.454349Z"
    },
    "papermill": {
     "duration": 0.086516,
     "end_time": "2023-11-24T21:35:37.457294",
     "exception": false,
     "start_time": "2023-11-24T21:35:37.370778",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "for col in df.select_dtypes('float64'):\n",
    "    df[col] = pd.to_numeric(df[col], downcast = 'float')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7524c569",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-24T21:35:37.469167Z",
     "iopub.status.busy": "2023-11-24T21:35:37.468889Z",
     "iopub.status.idle": "2023-11-24T21:35:37.476643Z",
     "shell.execute_reply": "2023-11-24T21:35:37.475909Z"
    },
    "papermill": {
     "duration": 0.015823,
     "end_time": "2023-11-24T21:35:37.478668",
     "exception": false,
     "start_time": "2023-11-24T21:35:37.462845",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Date             datetime64[ns]\n",
       "Location                 object\n",
       "MinTemp                 float32\n",
       "MaxTemp                 float32\n",
       "Rainfall                float32\n",
       "Evaporation             float32\n",
       "Sunshine                float32\n",
       "WindGustDir              object\n",
       "WindGustSpeed           float32\n",
       "WindDir9am               object\n",
       "WindDir3pm               object\n",
       "WindSpeed9am            float32\n",
       "WindSpeed3pm            float32\n",
       "Humidity9am             float32\n",
       "Humidity3pm             float32\n",
       "Pressure9am             float32\n",
       "Pressure3pm             float32\n",
       "Cloud9am                float32\n",
       "Cloud3pm                float32\n",
       "Temp9am                 float32\n",
       "Temp3pm                 float32\n",
       "RainToday               float32\n",
       "RainTomorrow            float32\n",
       "dtype: object"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "faac809e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-24T21:35:37.490492Z",
     "iopub.status.busy": "2023-11-24T21:35:37.490208Z",
     "iopub.status.idle": "2023-11-24T21:35:37.500769Z",
     "shell.execute_reply": "2023-11-24T21:35:37.499879Z"
    },
    "papermill": {
     "duration": 0.018704,
     "end_time": "2023-11-24T21:35:37.502710",
     "exception": false,
     "start_time": "2023-11-24T21:35:37.484006",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "X = df.select_dtypes('float')\n",
    "y = df.RainTomorrow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "133c2881",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-24T21:35:37.515033Z",
     "iopub.status.busy": "2023-11-24T21:35:37.514749Z",
     "iopub.status.idle": "2023-11-24T21:35:37.536161Z",
     "shell.execute_reply": "2023-11-24T21:35:37.535352Z"
    },
    "papermill": {
     "duration": 0.029672,
     "end_time": "2023-11-24T21:35:37.538112",
     "exception": false,
     "start_time": "2023-11-24T21:35:37.508440",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "da175568",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-24T21:35:37.551308Z",
     "iopub.status.busy": "2023-11-24T21:35:37.551017Z",
     "iopub.status.idle": "2023-11-24T21:35:37.826232Z",
     "shell.execute_reply": "2023-11-24T21:35:37.825236Z"
    },
    "papermill": {
     "duration": 0.285192,
     "end_time": "2023-11-24T21:35:37.828763",
     "exception": false,
     "start_time": "2023-11-24T21:35:37.543571",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/sklearn/utils/validation.py:767: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if not hasattr(array, \"sparse\") and array.dtypes.apply(is_sparse).any():\n",
      "/opt/conda/lib/python3.10/site-packages/sklearn/utils/validation.py:605: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if is_sparse(pd_dtype):\n",
      "/opt/conda/lib/python3.10/site-packages/sklearn/utils/validation.py:614: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if is_sparse(pd_dtype) or not is_extension_array_dtype(pd_dtype):\n",
      "/opt/conda/lib/python3.10/site-packages/sklearn/utils/validation.py:767: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if not hasattr(array, \"sparse\") and array.dtypes.apply(is_sparse).any():\n",
      "/opt/conda/lib/python3.10/site-packages/sklearn/utils/validation.py:605: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if is_sparse(pd_dtype):\n",
      "/opt/conda/lib/python3.10/site-packages/sklearn/utils/validation.py:614: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if is_sparse(pd_dtype) or not is_extension_array_dtype(pd_dtype):\n",
      "/opt/conda/lib/python3.10/site-packages/sklearn/utils/validation.py:767: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if not hasattr(array, \"sparse\") and array.dtypes.apply(is_sparse).any():\n",
      "/opt/conda/lib/python3.10/site-packages/sklearn/utils/validation.py:605: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if is_sparse(pd_dtype):\n",
      "/opt/conda/lib/python3.10/site-packages/sklearn/utils/validation.py:614: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if is_sparse(pd_dtype) or not is_extension_array_dtype(pd_dtype):\n",
      "/opt/conda/lib/python3.10/site-packages/sklearn/utils/validation.py:767: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if not hasattr(array, \"sparse\") and array.dtypes.apply(is_sparse).any():\n",
      "/opt/conda/lib/python3.10/site-packages/sklearn/utils/validation.py:605: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if is_sparse(pd_dtype):\n",
      "/opt/conda/lib/python3.10/site-packages/sklearn/utils/validation.py:614: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if is_sparse(pd_dtype) or not is_extension_array_dtype(pd_dtype):\n"
     ]
    }
   ],
   "source": [
    "imputer = SimpleImputer(strategy='median')\n",
    "X_train = imputer.fit_transform(X_train)\n",
    "X_test = imputer.fit_transform(X_test)\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.fit_transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f097440f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-24T21:35:37.845245Z",
     "iopub.status.busy": "2023-11-24T21:35:37.844916Z",
     "iopub.status.idle": "2023-11-24T21:35:37.849649Z",
     "shell.execute_reply": "2023-11-24T21:35:37.848861Z"
    },
    "papermill": {
     "duration": 0.013616,
     "end_time": "2023-11-24T21:35:37.851486",
     "exception": false,
     "start_time": "2023-11-24T21:35:37.837870",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "X_train = X_train.reshape((X_train.shape[0], X_train.shape[1], 1))\n",
    "X_test = X_test.reshape((X_test.shape[0], X_test.shape[1], 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7bdefea0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-24T21:35:37.863889Z",
     "iopub.status.busy": "2023-11-24T21:35:37.863192Z",
     "iopub.status.idle": "2023-11-24T21:35:37.867631Z",
     "shell.execute_reply": "2023-11-24T21:35:37.866818Z"
    },
    "papermill": {
     "duration": 0.012546,
     "end_time": "2023-11-24T21:35:37.869562",
     "exception": false,
     "start_time": "2023-11-24T21:35:37.857016",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "generator_train = TimeseriesGenerator(X_train, y_train, length=1, batch_size=1)\n",
    "generator_test = TimeseriesGenerator(X_test, y_test, length=1, batch_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "893139cb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-24T21:35:37.881965Z",
     "iopub.status.busy": "2023-11-24T21:35:37.881702Z",
     "iopub.status.idle": "2023-11-24T21:35:41.112217Z",
     "shell.execute_reply": "2023-11-24T21:35:41.111220Z"
    },
    "papermill": {
     "duration": 3.239292,
     "end_time": "2023-11-24T21:35:41.114574",
     "exception": false,
     "start_time": "2023-11-24T21:35:37.875282",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(LSTM(32, input_shape=X_train.shape[1:], return_sequences=True))\n",
    "model.add(LSTM(32))\n",
    "\n",
    "model.add(Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5599494d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-24T21:35:41.127537Z",
     "iopub.status.busy": "2023-11-24T21:35:41.127215Z",
     "iopub.status.idle": "2023-11-24T21:35:41.150816Z",
     "shell.execute_reply": "2023-11-24T21:35:41.150091Z"
    },
    "papermill": {
     "duration": 0.032288,
     "end_time": "2023-11-24T21:35:41.152844",
     "exception": false,
     "start_time": "2023-11-24T21:35:41.120556",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=[BinaryAccuracy()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "173c9330",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-24T21:35:41.165242Z",
     "iopub.status.busy": "2023-11-24T21:35:41.164951Z",
     "iopub.status.idle": "2023-11-24T21:35:41.182609Z",
     "shell.execute_reply": "2023-11-24T21:35:41.181798Z"
    },
    "papermill": {
     "duration": 0.027635,
     "end_time": "2023-11-24T21:35:41.186169",
     "exception": false,
     "start_time": "2023-11-24T21:35:41.158534",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm (LSTM)                 (None, 18, 32)            4352      \n",
      "                                                                 \n",
      " lstm_1 (LSTM)               (None, 32)                8320      \n",
      "                                                                 \n",
      " dense (Dense)               (None, 1)                 33        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 12705 (49.63 KB)\n",
      "Trainable params: 12705 (49.63 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "aa9530d9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-24T21:35:41.200728Z",
     "iopub.status.busy": "2023-11-24T21:35:41.200184Z",
     "iopub.status.idle": "2023-11-24T21:35:41.204534Z",
     "shell.execute_reply": "2023-11-24T21:35:41.203712Z"
    },
    "papermill": {
     "duration": 0.013701,
     "end_time": "2023-11-24T21:35:41.206468",
     "exception": false,
     "start_time": "2023-11-24T21:35:41.192767",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "filepath = \"weights_32_2layers.hdf5\"\n",
    "checkpoint = ModelCheckpoint(filepath, \n",
    "                             monitor='loss', \n",
    "                             verbose=1, \n",
    "                             save_best_only=True, \n",
    "                             mode='min')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3d0148e5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-24T21:35:41.221080Z",
     "iopub.status.busy": "2023-11-24T21:35:41.220815Z",
     "iopub.status.idle": "2023-11-24T21:35:41.227629Z",
     "shell.execute_reply": "2023-11-24T21:35:41.226815Z"
    },
    "papermill": {
     "duration": 0.016646,
     "end_time": "2023-11-24T21:35:41.229711",
     "exception": false,
     "start_time": "2023-11-24T21:35:41.213065",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/device:GPU:0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(tf.test.gpu_device_name())\n",
    "# See https://www.tensorflow.org/tutorials/using_gpu#allowing_gpu_memory_growth\n",
    "config = tf.compat.v1.ConfigProto()\n",
    "config.gpu_options.allow_growth = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c2ad03e9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-24T21:35:41.244207Z",
     "iopub.status.busy": "2023-11-24T21:35:41.243934Z",
     "iopub.status.idle": "2023-11-24T22:20:01.796339Z",
     "shell.execute_reply": "2023-11-24T22:20:01.795348Z"
    },
    "papermill": {
     "duration": 2660.581308,
     "end_time": "2023-11-24T22:20:01.817667",
     "exception": false,
     "start_time": "2023-11-24T21:35:41.236359",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1: loss improved from inf to 0.05472, saving model to weights_32_2layers.hdf5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/keras/src/engine/training.py:3000: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 2: loss improved from 0.05472 to 0.00018, saving model to weights_32_2layers.hdf5\n",
      "\n",
      "Epoch 3: loss improved from 0.00018 to 0.00004, saving model to weights_32_2layers.hdf5\n",
      "\n",
      "Epoch 4: loss improved from 0.00004 to 0.00001, saving model to weights_32_2layers.hdf5\n",
      "\n",
      "Epoch 5: loss improved from 0.00001 to 0.00000, saving model to weights_32_2layers.hdf5\n",
      "\n",
      "Epoch 6: loss improved from 0.00000 to 0.00000, saving model to weights_32_2layers.hdf5\n",
      "\n",
      "Epoch 7: loss improved from 0.00000 to 0.00000, saving model to weights_32_2layers.hdf5\n",
      "\n",
      "Epoch 8: loss improved from 0.00000 to 0.00000, saving model to weights_32_2layers.hdf5\n",
      "\n",
      "Epoch 9: loss improved from 0.00000 to 0.00000, saving model to weights_32_2layers.hdf5\n",
      "\n",
      "Epoch 10: loss improved from 0.00000 to 0.00000, saving model to weights_32_2layers.hdf5\n",
      "\n",
      "Epoch 11: loss improved from 0.00000 to 0.00000, saving model to weights_32_2layers.hdf5\n",
      "\n",
      "Epoch 12: loss improved from 0.00000 to 0.00000, saving model to weights_32_2layers.hdf5\n",
      "\n",
      "Epoch 13: loss improved from 0.00000 to 0.00000, saving model to weights_32_2layers.hdf5\n",
      "\n",
      "Epoch 14: loss improved from 0.00000 to 0.00000, saving model to weights_32_2layers.hdf5\n",
      "\n",
      "Epoch 15: loss improved from 0.00000 to 0.00000, saving model to weights_32_2layers.hdf5\n",
      "\n",
      "Epoch 16: loss improved from 0.00000 to 0.00000, saving model to weights_32_2layers.hdf5\n",
      "\n",
      "Epoch 17: loss improved from 0.00000 to 0.00000, saving model to weights_32_2layers.hdf5\n",
      "\n",
      "Epoch 18: loss improved from 0.00000 to 0.00000, saving model to weights_32_2layers.hdf5\n",
      "\n",
      "Epoch 19: loss improved from 0.00000 to 0.00000, saving model to weights_32_2layers.hdf5\n",
      "\n",
      "Epoch 20: loss improved from 0.00000 to 0.00000, saving model to weights_32_2layers.hdf5\n",
      "\n",
      "Epoch 21: loss improved from 0.00000 to 0.00000, saving model to weights_32_2layers.hdf5\n",
      "\n",
      "Epoch 22: loss improved from 0.00000 to 0.00000, saving model to weights_32_2layers.hdf5\n",
      "\n",
      "Epoch 23: loss improved from 0.00000 to 0.00000, saving model to weights_32_2layers.hdf5\n",
      "\n",
      "Epoch 24: loss improved from 0.00000 to 0.00000, saving model to weights_32_2layers.hdf5\n",
      "\n",
      "Epoch 25: loss improved from 0.00000 to 0.00000, saving model to weights_32_2layers.hdf5\n",
      "\n",
      "Epoch 26: loss improved from 0.00000 to 0.00000, saving model to weights_32_2layers.hdf5\n",
      "\n",
      "Epoch 27: loss improved from 0.00000 to 0.00000, saving model to weights_32_2layers.hdf5\n",
      "\n",
      "Epoch 28: loss improved from 0.00000 to 0.00000, saving model to weights_32_2layers.hdf5\n",
      "\n",
      "Epoch 29: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 30: loss improved from 0.00000 to 0.00000, saving model to weights_32_2layers.hdf5\n",
      "\n",
      "Epoch 31: loss improved from 0.00000 to 0.00000, saving model to weights_32_2layers.hdf5\n",
      "\n",
      "Epoch 32: loss improved from 0.00000 to 0.00000, saving model to weights_32_2layers.hdf5\n",
      "\n",
      "Epoch 33: loss improved from 0.00000 to 0.00000, saving model to weights_32_2layers.hdf5\n",
      "\n",
      "Epoch 34: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 35: loss improved from 0.00000 to 0.00000, saving model to weights_32_2layers.hdf5\n",
      "\n",
      "Epoch 36: loss improved from 0.00000 to 0.00000, saving model to weights_32_2layers.hdf5\n",
      "\n",
      "Epoch 37: loss improved from 0.00000 to 0.00000, saving model to weights_32_2layers.hdf5\n",
      "\n",
      "Epoch 38: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 39: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 40: loss improved from 0.00000 to 0.00000, saving model to weights_32_2layers.hdf5\n",
      "\n",
      "Epoch 41: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 42: loss improved from 0.00000 to 0.00000, saving model to weights_32_2layers.hdf5\n",
      "\n",
      "Epoch 43: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 44: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 45: loss improved from 0.00000 to 0.00000, saving model to weights_32_2layers.hdf5\n",
      "\n",
      "Epoch 46: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 47: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 48: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 49: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 50: loss improved from 0.00000 to 0.00000, saving model to weights_32_2layers.hdf5\n",
      "\n",
      "Epoch 51: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 52: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 53: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 54: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 55: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 56: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 57: loss improved from 0.00000 to 0.00000, saving model to weights_32_2layers.hdf5\n",
      "\n",
      "Epoch 58: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 59: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 60: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 61: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 62: loss improved from 0.00000 to 0.00000, saving model to weights_32_2layers.hdf5\n",
      "\n",
      "Epoch 63: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 64: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 65: loss improved from 0.00000 to 0.00000, saving model to weights_32_2layers.hdf5\n",
      "\n",
      "Epoch 66: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 67: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 68: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 69: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 70: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 71: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 72: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 73: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 74: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 75: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 76: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 77: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 78: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 79: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 80: loss improved from 0.00000 to 0.00000, saving model to weights_32_2layers.hdf5\n",
      "\n",
      "Epoch 81: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 82: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 83: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 84: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 85: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 86: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 87: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 88: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 89: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 90: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 91: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 92: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 93: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 94: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 95: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 96: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 97: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 98: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 99: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 100: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 101: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 102: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 103: loss improved from 0.00000 to 0.00000, saving model to weights_32_2layers.hdf5\n",
      "\n",
      "Epoch 104: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 105: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 106: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 107: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 108: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 109: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 110: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 111: loss improved from 0.00000 to 0.00000, saving model to weights_32_2layers.hdf5\n",
      "\n",
      "Epoch 112: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 113: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 114: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 115: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 116: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 117: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 118: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 119: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 120: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 121: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 122: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 123: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 124: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 125: loss improved from 0.00000 to 0.00000, saving model to weights_32_2layers.hdf5\n",
      "\n",
      "Epoch 126: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 127: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 128: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 129: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 130: loss improved from 0.00000 to 0.00000, saving model to weights_32_2layers.hdf5\n",
      "\n",
      "Epoch 131: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 132: loss improved from 0.00000 to 0.00000, saving model to weights_32_2layers.hdf5\n",
      "\n",
      "Epoch 133: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 134: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 135: loss improved from 0.00000 to 0.00000, saving model to weights_32_2layers.hdf5\n",
      "\n",
      "Epoch 136: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 137: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 138: loss improved from 0.00000 to 0.00000, saving model to weights_32_2layers.hdf5\n",
      "\n",
      "Epoch 139: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 140: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 141: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 142: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 143: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 144: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 145: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 146: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 147: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 148: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 149: loss improved from 0.00000 to 0.00000, saving model to weights_32_2layers.hdf5\n",
      "\n",
      "Epoch 150: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 151: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 152: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 153: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 154: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 155: loss improved from 0.00000 to 0.00000, saving model to weights_32_2layers.hdf5\n",
      "\n",
      "Epoch 156: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 157: loss improved from 0.00000 to 0.00000, saving model to weights_32_2layers.hdf5\n",
      "\n",
      "Epoch 158: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 159: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 160: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 161: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 162: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 163: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 164: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 165: loss improved from 0.00000 to 0.00000, saving model to weights_32_2layers.hdf5\n",
      "\n",
      "Epoch 166: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 167: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 168: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 169: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 170: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 171: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 172: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 173: loss improved from 0.00000 to 0.00000, saving model to weights_32_2layers.hdf5\n",
      "\n",
      "Epoch 174: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 175: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 176: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 177: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 178: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 179: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 180: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 181: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 182: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 183: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 184: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 185: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 186: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 187: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 188: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 189: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 190: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 191: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 192: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 193: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 194: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 195: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 196: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 197: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 198: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 199: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 200: loss did not improve from 0.00000\n"
     ]
    }
   ],
   "source": [
    "with tf.device('/gpu:0'):\n",
    "    model.fit(X_train, y_train, batch_size=32, epochs=200, verbose=0, validation_split=0.2,\n",
    "             callbacks=[checkpoint])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "57b0030f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-24T22:20:01.863487Z",
     "iopub.status.busy": "2023-11-24T22:20:01.862733Z",
     "iopub.status.idle": "2023-11-24T22:20:06.313532Z",
     "shell.execute_reply": "2023-11-24T22:20:06.312493Z"
    },
    "papermill": {
     "duration": 4.476081,
     "end_time": "2023-11-24T22:20:06.315724",
     "exception": false,
     "start_time": "2023-11-24T22:20:01.839643",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1467/1467 [==============================] - 4s 3ms/step - loss: 2.4602e-09 - binary_accuracy: 1.0000\n"
     ]
    }
   ],
   "source": [
    "loss, acc = model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "42183856",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-24T22:20:06.375178Z",
     "iopub.status.busy": "2023-11-24T22:20:06.374848Z",
     "iopub.status.idle": "2023-11-24T22:20:06.380565Z",
     "shell.execute_reply": "2023-11-24T22:20:06.379792Z"
    },
    "papermill": {
     "duration": 0.037147,
     "end_time": "2023-11-24T22:20:06.382403",
     "exception": false,
     "start_time": "2023-11-24T22:20:06.345256",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2.460221137923213e-09, 1.0)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss, acc"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 6012,
     "sourceId": 1733506,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30587,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 2689.619548,
   "end_time": "2023-11-24T22:20:09.596626",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2023-11-24T21:35:19.977078",
   "version": "2.4.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
